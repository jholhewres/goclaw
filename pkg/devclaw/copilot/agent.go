// Package copilot ‚Äì agent.go implements the agentic loop that orchestrates
// LLM calls with tool execution. The agent iterates: call LLM ‚Üí if tool_calls
// ‚Üí execute tools ‚Üí append results ‚Üí call LLM again, until the LLM produces
// a final text response with no tool calls.
//
// Architecture:
//   - No fixed max turns ‚Äî the loop runs until the LLM stops calling tools.
//   - Single run timeout (default: 600s = 10min) controls the whole run.
//   - Per-LLM-call safety timeout (5min) prevents individual hung requests.
//   - Reflection nudge every 15 turns for budget awareness.
//   - Auto-compaction on context overflow (up to 3 attempts).
package copilot

import (
	"context"
	"fmt"
	"log/slog"
	"strings"
	"time"
)

const (
	// DefaultRunTimeout is the maximum duration for an entire agent run.
	// Set to 20 minutes to accommodate coding tasks that invoke Claude Code CLI
	// (which itself can take 5-15 minutes for complex projects).
	// This is the PRIMARY timeout ‚Äî no per-turn limit.
	DefaultRunTimeout = 1200 * time.Second

	// DefaultLLMCallTimeout is the safety-net timeout for a single LLM API call.
	// This only prevents hung HTTP connections ‚Äî it should be generous enough
	// that even large contexts complete. 5 minutes covers worst-case scenarios.
	DefaultLLMCallTimeout = 5 * time.Minute

	// reflectionInterval is how often (in turns) the agent receives a budget nudge.
	// Reduced from 15 to 5 to catch stuck patterns earlier.
	reflectionInterval = 5

	// DefaultMaxCompactionAttempts is how many times to retry after context overflow compaction.
	DefaultMaxCompactionAttempts = 3
)

// AgentConfig holds configurable agent loop parameters.
type AgentConfig struct {
	// RunTimeoutSeconds is the max seconds for the entire agent run (default: 600).
	// One timer for the whole run, not per-turn.
	RunTimeoutSeconds int `yaml:"run_timeout_seconds"`

	// LLMCallTimeoutSeconds is the safety-net timeout per individual LLM call
	// (default: 300). Only catches hung connections ‚Äî not the primary timeout.
	LLMCallTimeoutSeconds int `yaml:"llm_call_timeout_seconds"`

	// MaxTurns is a soft safety limit on LLM round-trips (default: 0 = unlimited).
	// When > 0, the agent will request a summary after this many turns.
	MaxTurns int `yaml:"max_turns"`

	// MaxContinuations is how many auto-continue rounds are allowed when
	// MaxTurns is hit and the agent is still using tools.
	// Only relevant when MaxTurns > 0. Default: 2.
	MaxContinuations int `yaml:"max_continuations"`

	// ReflectionEnabled enables periodic budget awareness nudges (default: true).
	ReflectionEnabled bool `yaml:"reflection_enabled"`

	// MaxCompactionAttempts is how many times to retry after context overflow (default: 3).
	MaxCompactionAttempts int `yaml:"max_compaction_attempts"`

	// ToolLoop configures tool loop detection thresholds.
	ToolLoop ToolLoopConfig `yaml:"tool_loop"`
}

// DefaultAgentConfig returns sensible defaults for agent autonomy.
func DefaultAgentConfig() AgentConfig {
	return AgentConfig{
		RunTimeoutSeconds:     int(DefaultRunTimeout / time.Second),
		LLMCallTimeoutSeconds: int(DefaultLLMCallTimeout / time.Second),
		MaxTurns:              0, // Unlimited
		MaxContinuations:      2,
		ReflectionEnabled:     true,
		MaxCompactionAttempts: DefaultMaxCompactionAttempts,
	}
}

// AgentRun encapsulates a single agent execution with its dependencies.
type AgentRun struct {
	llm                   *LLMClient
	executor              *ToolExecutor
	runTimeout            time.Duration // Total run timeout (default: 600s)
	llmCallTimeout        time.Duration // Per-LLM-call safety timeout (default: 5min)
	maxTurns              int           // 0 = unlimited
	reflectionOn          bool
	maxCompactionAttempts int
	streamCallback        StreamCallback
	modelOverride         string                             // When set, use this model instead of default.
	usageRecorder         func(model string, usage LLMUsage) // Called after each successful LLM response.

	// interruptCh receives follow-up user messages that should be injected into
	// the active agent loop. Between turns, the agent drains this channel and
	// appends the messages to the conversation before the next LLM call.
	// This enables Claude Code-style live message injection.
	interruptCh <-chan string

	// onBeforeToolExec is called right before tool execution starts.
	// Used to flush any buffered stream text so the user sees the LLM's
	// intermediate reasoning before tools run.
	onBeforeToolExec func()

	// onToolResult is called after each tool execution completes.
	// Used to auto-send media (e.g. generated images) to the channel.
	onToolResult func(name string, result ToolResult)

	// loopDetector tracks tool call history and detects repetitive patterns.
	loopDetector *ToolLoopDetector

	logger *slog.Logger
}

// NewAgentRun creates a new agent runner.
func NewAgentRun(llm *LLMClient, executor *ToolExecutor, logger *slog.Logger) *AgentRun {
	return &AgentRun{
		llm:                   llm,
		executor:              executor,
		runTimeout:            DefaultRunTimeout,
		llmCallTimeout:        DefaultLLMCallTimeout,
		maxTurns:              0, // Unlimited
		reflectionOn:          true,
		maxCompactionAttempts: DefaultMaxCompactionAttempts,
		logger:                logger.With("component", "agent"),
	}
}

// NewAgentRunWithConfig creates a new agent runner with explicit configuration.
func NewAgentRunWithConfig(llm *LLMClient, executor *ToolExecutor, cfg AgentConfig, logger *slog.Logger) *AgentRun {
	ar := NewAgentRun(llm, executor, logger)
	if cfg.RunTimeoutSeconds > 0 {
		ar.runTimeout = time.Duration(cfg.RunTimeoutSeconds) * time.Second
	}
	if cfg.LLMCallTimeoutSeconds > 0 {
		ar.llmCallTimeout = time.Duration(cfg.LLMCallTimeoutSeconds) * time.Second
	}
	if cfg.MaxTurns >= 0 {
		ar.maxTurns = cfg.MaxTurns // 0 = unlimited
	}
	ar.reflectionOn = cfg.ReflectionEnabled
	if cfg.MaxCompactionAttempts > 0 {
		ar.maxCompactionAttempts = cfg.MaxCompactionAttempts
	}
	return ar
}

// SetStreamCallback sets the callback for streaming text deltas.
// When set, the agent uses CompleteWithToolsStream; only text content is forwarded,
// tool calls are accumulated silently.
func (a *AgentRun) SetStreamCallback(cb StreamCallback) {
	a.streamCallback = cb
}

// SetModelOverride sets the model to use instead of the default.
// Empty string means use the LLM client's default.
func (a *AgentRun) SetModelOverride(model string) {
	a.modelOverride = model
}

// SetUsageRecorder sets a callback invoked after each successful LLM response.
func (a *AgentRun) SetUsageRecorder(fn func(model string, usage LLMUsage)) {
	a.usageRecorder = fn
}

// SetOnBeforeToolExec sets a callback fired right before tool execution starts
// in the agent loop. Used by the block streamer to flush buffered text so the
// user sees intermediate reasoning before tools run.
func (a *AgentRun) SetOnBeforeToolExec(fn func()) {
	a.onBeforeToolExec = fn
}

// SetOnToolResult sets a callback fired after each tool execution completes.
// Used to auto-send media (e.g. generated images) to the channel.
func (a *AgentRun) SetOnToolResult(fn func(name string, result ToolResult)) {
	a.onToolResult = fn
}

// SetLoopDetector sets the tool loop detector for this run.
func (a *AgentRun) SetLoopDetector(d *ToolLoopDetector) {
	a.loopDetector = d
}

// SetInterruptChannel sets the channel for receiving follow-up user messages
// during agent execution. Messages received on this channel are injected into
// the conversation between agent turns, allowing users to steer the agent
// mid-run (similar to Claude Code behavior).
func (a *AgentRun) SetInterruptChannel(ch <-chan string) {
	a.interruptCh = ch
}

// Run executes the agent loop: builds the initial message list from conversation
// history, then iterates LLM calls and tool executions until a final response
// is produced or the turn limit is exhausted.
//
// If auto-continue is enabled and the agent is still using tools when the
// budget runs out, it will automatically start a continuation round.
func (a *AgentRun) Run(ctx context.Context, systemPrompt string, history []ConversationEntry, userMessage string) (string, error) {
	content, _, err := a.RunWithUsage(ctx, systemPrompt, history, userMessage)
	return content, err
}

// RunWithUsage is like Run but also returns aggregated token usage from all LLM calls.
//
// Architecture:
//   - The loop runs until the LLM produces a response with no tool calls.
//   - A single run-level timeout controls the entire execution (default: 600s).
//   - Individual LLM calls have a safety-net timeout (5min) to catch hung connections.
//   - No fixed turn limit ‚Äî the agent keeps going as long as it has tools to call.
func (a *AgentRun) RunWithUsage(ctx context.Context, systemPrompt string, history []ConversationEntry, userMessage string) (string, *LLMUsage, error) {
	// ‚îÄ‚îÄ Run-level timeout (single timer for the whole run) ‚îÄ‚îÄ
	runCtx, runCancel := context.WithTimeout(ctx, a.runTimeout)
	defer runCancel()

	runStart := time.Now()

	// Build initial messages from history.
	messages := a.buildMessages(systemPrompt, history, userMessage)

	// Collect tool definitions from the executor, filtered by profile if present.
	allTools := a.executor.Tools()
	var tools []ToolDefinition

	profile := ToolProfileFromContext(runCtx)
	if profile != nil {
		// Filter tools by profile
		allToolNames := a.executor.ToolNames()
		checker := NewProfileChecker(profile.Allow, profile.Deny, allToolNames)
		for _, tool := range allTools {
			if allowed, _ := checker.Check(tool.Function.Name); allowed {
				tools = append(tools, tool)
			}
		}
		a.logger.Debug("tools filtered by profile",
			"profile", profile.Name,
			"total_tools", len(allTools),
			"allowed_tools", len(tools),
		)
	} else {
		tools = allTools
	}

	// Limit tools to 128 for OpenAI API compatibility
	const maxTools = 128
	if len(tools) > maxTools {
		a.logger.Warn("too many tools, truncating to max",
			"total", len(tools),
			"max", maxTools,
		)
		tools = tools[:maxTools]
	}

	a.logger.Debug("agent run started",
		"history_entries", len(history),
		"tools_available", len(tools),
		"run_timeout_s", int(a.runTimeout.Seconds()),
		"max_turns", a.maxTurns,
	)

	// If no tools are registered, do a single completion and return.
	if len(tools) == 0 {
		resp, err := a.doLLMCallWithOverflowRetry(runCtx, messages, nil)
		if err != nil {
			return "", nil, err
		}
		var totalUsage LLMUsage
		a.accumulateUsage(&totalUsage, resp)
		return resp.Content, &totalUsage, nil
	}

	var totalUsage LLMUsage
	totalTurns := 0

	// Progress cooldown: avoid flooding the user with tool progress messages.
	// Short 3s cooldown for faster feedback while avoiding message spam.
	const progressCooldown = 3 * time.Second
	var lastProgressAt time.Time

	// ‚îÄ‚îÄ Main agent loop ‚îÄ‚îÄ
	// Loop until: (1) LLM produces no tool calls, (2) run timeout fires, or
	// (3) optional soft turn limit is hit. No fixed turn limit by default.
	for {
		totalTurns++
		turnStart := time.Now()

		a.logger.Debug("agent turn start",
			"turn", totalTurns,
			"messages", len(messages),
			"run_elapsed_s", int(time.Since(runStart).Seconds()),
		)

		// ‚îÄ‚îÄ Soft turn limit (optional, 0 = disabled) ‚îÄ‚îÄ
		if a.maxTurns > 0 && totalTurns > a.maxTurns {
			a.logger.Warn("agent reached soft turn limit, requesting summary",
				"total_turns", totalTurns,
				"max_turns", a.maxTurns,
			)
			messages = append(messages, chatMessage{
				Role: "user",
				Content: "[System: You have used many turns. " +
					"Please provide your best response with the information gathered so far.]",
			})
			resp, err := a.doLLMCallWithOverflowRetry(runCtx, messages, nil)
			if err != nil {
				return "", nil, fmt.Errorf("final summary call failed: %w", err)
			}
			a.accumulateUsage(&totalUsage, resp)
			return resp.Content, &totalUsage, nil
		}

		// ‚îÄ‚îÄ Run timeout check ‚îÄ‚îÄ
		if runCtx.Err() != nil {
			return "", &totalUsage, fmt.Errorf("agent run timeout (%s) after %d turns: %w",
				a.runTimeout, totalTurns, runCtx.Err())
		}

		// ‚îÄ‚îÄ Interrupt injection ‚îÄ‚îÄ
		// Check for follow-up user messages sent while the agent was working.
		if totalTurns > 1 {
			if interrupts := a.drainInterrupts(); len(interrupts) > 0 {
				for _, interrupt := range interrupts {
					messages = append(messages, chatMessage{
						Role:    "user",
						Content: "[Follow-up from user while processing]\n" + interrupt,
					})
				}
				a.logger.Info("injected interrupt messages into agent loop",
					"count", len(interrupts),
					"turn", totalTurns,
				)
			}
		}

		// ‚îÄ‚îÄ Proactive context compaction ‚îÄ‚îÄ
		// Instead of dropping old messages entirely (which causes amnesia), we
		// compact the context if it grows too large.
		if totalTurns > 5 && len(messages) > 15 {
			a.logger.Debug("checking context size for compaction", "messages_len", len(messages))
			messages = a.managedCompaction(runCtx, messages)
		}

		// Inject reflection nudge periodically so the agent is aware of duration.
		// More aggressive messaging to catch stuck patterns early.
		if a.reflectionOn && totalTurns > 1 && totalTurns%reflectionInterval == 0 {
			elapsed := time.Since(runStart).Seconds()
			remaining := a.runTimeout.Seconds() - elapsed
			messages = append(messages, chatMessage{
				Role: "user",
				Content: fmt.Sprintf(
					"[System: Turn %d checkpoint (%.0fs elapsed, ~%.0fs remaining). "+
						"If you're stuck or repeating the same approach, STOP and investigate the root cause. "+
						"Don't repeat failed approaches ‚Äî think before acting.]",
					totalTurns, elapsed, remaining,
				),
			})
		}

		// ‚îÄ‚îÄ Call LLM ‚îÄ‚îÄ
		llmStart := time.Now()
		resp, err := a.doLLMCallWithOverflowRetry(runCtx, messages, tools)
		llmDuration := time.Since(llmStart)
		if err != nil {
			// If the parent/run context was cancelled, propagate immediately.
			if runCtx.Err() != nil {
				// Distinguish user abort from run timeout.
				if ctx.Err() != nil {
					return "", &totalUsage, fmt.Errorf("agent cancelled by user: %w", ctx.Err())
				}
				return "", &totalUsage, fmt.Errorf("agent run timeout (%s) at turn %d: %w",
					a.runTimeout, totalTurns, runCtx.Err())
			}

			// Timeout or transient error on a later turn: try compacting
			// the context and retrying once before giving up.
			errStr := err.Error()
			isTimeout := strings.Contains(errStr, "deadline exceeded") || strings.Contains(errStr, "context canceled") || strings.Contains(errStr, "rate limit")

			if isTimeout && totalTurns > 2 && len(messages) > 10 {
				a.logger.Warn("LLM call timed out or rate limited, aggressive compaction and retry",
					"turn", totalTurns,
					"messages_before", len(messages),
					"llm_ms", llmDuration.Milliseconds(),
				)
				messages = a.aggressiveCompaction(runCtx, messages)

				// Retry the LLM call with compacted context.
				llmStart = time.Now()
				resp, err = a.doLLMCallWithOverflowRetry(runCtx, messages, tools)
				llmDuration = time.Since(llmStart)
			}

			if err != nil {
				return "", &totalUsage, fmt.Errorf("LLM call failed (turn %d, llm_ms=%d): %w",
					totalTurns, llmDuration.Milliseconds(), err)
			}
		}
		a.accumulateUsage(&totalUsage, resp)

		a.logger.Info("LLM call complete",
			"turn", totalTurns,
			"llm_ms", llmDuration.Milliseconds(),
			"tool_calls", len(resp.ToolCalls),
			"prompt_tokens", resp.Usage.PromptTokens,
			"completion_tokens", resp.Usage.CompletionTokens,
		)

		// ‚îÄ‚îÄ Strict <think> Parsing ‚îÄ‚îÄ
		if strings.Contains(resp.Content, "<think>") && !strings.Contains(resp.Content, "</think>") {
			a.logger.Warn("llm missed closing </think> tag, prompting retry without executing tools")

			// Append assistant message so the user message makes sense in context
			messages = append(messages, chatMessage{
				Role:      "assistant",
				Content:   resp.Content,
				ToolCalls: resp.ToolCalls,
			})

			messages = append(messages, chatMessage{
				Role:    "user",
				Content: "[System: You opened a <think> tag but did not close it with </think>. Please close your <think> tag, and place any tool calls or final responses AFTER the </think> tag. Do not execute tools until you finish thinking.]",
			})
			// Loop again without executing any returned tool calls or triggering final response
			continue
		}

		// ‚îÄ‚îÄ No tool calls ‚Üí final response ‚îÄ‚îÄ
		if len(resp.ToolCalls) == 0 {
			a.logger.Info("agent completed",
				"total_turns", totalTurns,
				"response_len", len(resp.Content),
				"run_elapsed_ms", time.Since(runStart).Milliseconds(),
			)
			return resp.Content, &totalUsage, nil
		}

		// Append assistant message with tool calls to the conversation.
		messages = append(messages, chatMessage{
			Role:      "assistant",
			Content:   resp.Content,
			ToolCalls: resp.ToolCalls,
		})

		// ‚îÄ‚îÄ Tool Loop Detection ‚îÄ‚îÄ
		// Record tool calls and check for repetitive patterns before execution.
		// Warnings/criticals are deferred until AFTER tool results to maintain
		// valid message ordering (assistant‚Üítool‚Üíuser, not assistant‚Üíuser‚Üítool).
		var loopWarning string
		if a.loopDetector != nil {
			for _, tc := range resp.ToolCalls {
				args, _ := parseToolArgs(tc.Function.Arguments)
				result := a.loopDetector.RecordAndCheck(tc.Function.Name, args)

				switch result.Severity {
				case LoopBreaker:
					a.logger.Error("tool loop circuit breaker",
						"tool", tc.Function.Name, "streak", result.Streak, "pattern", result.Pattern)
					return result.Message, &totalUsage, nil

				case LoopCritical, LoopWarning:
					loopWarning = result.Message
				}
			}
		}

		// Execute all requested tool calls.
		toolStart := time.Now()
		toolNames := make([]string, len(resp.ToolCalls))
		for i, tc := range resp.ToolCalls {
			toolNames[i] = tc.Function.Name
		}
		a.logger.Info("executing tool calls",
			"count", len(resp.ToolCalls),
			"tools", strings.Join(toolNames, ","),
			"turn", totalTurns,
		)

		// Flush any buffered stream text before tools start ‚Äî ensures the user
		// sees the LLM's intermediate reasoning/thoughts immediately.
		if a.onBeforeToolExec != nil {
			a.onBeforeToolExec()
		}

		// Send progress to the user so they see what the agent is doing
		// while tools execute (especially for long-running tools).
		// When a stream callback (BlockStreamer) is active, it already delivers
		// the LLM's text progressively ‚Äî sending resp.Content via ProgressSender
		// would duplicate messages. Only send tool descriptions as progress.
		if ps := ProgressSenderFromContext(runCtx); ps != nil {
			now := time.Now()
			if now.Sub(lastProgressAt) >= progressCooldown {
				var progressMsg string

				if a.streamCallback != nil {
					// BlockStreamer is active and already sent resp.Content
					// to the channel ‚Äî only send a tool description as progress.
					progressMsg = formatToolProgressMessage(resp.ToolCalls)
				} else if resp.Content != "" && len(resp.Content) < 1000 {
					// No streamer ‚Äî send the LLM's own text as progress.
					progressMsg = resp.Content
				} else if resp.Content != "" {
					progressMsg = resp.Content[:500] + "..."
				} else {
					progressMsg = formatToolProgressMessage(resp.ToolCalls)
				}

				if progressMsg != "" {
					ps(runCtx, progressMsg)
					lastProgressAt = now
				}
			}
		}

		results := a.executor.Execute(runCtx, resp.ToolCalls)

		a.logger.Info("tool calls complete",
			"count", len(results),
			"tools_ms", time.Since(toolStart).Milliseconds(),
			"turn_ms", time.Since(turnStart).Milliseconds(),
		)

		// Append each tool result as a message.
		// Classify recoverable errors: the model should retry silently without
		// the user seeing transient failures.
		for _, result := range results {
			content := result.Content
			if result.Error != nil && isRecoverableToolError(content) {
				a.logger.Debug("recoverable tool error (model should retry)",
					"tool", result.Name,
					"error_preview", truncateStr(content, 80),
				)
			}
			messages = append(messages, chatMessage{
				Role:       "tool",
				Content:    content,
				ToolCallID: result.ToolCallID,
			})

			// Track tool output for progress-aware loop detection.
			if a.loopDetector != nil {
				a.loopDetector.RecordToolOutcome(content)
			}

			// Notify hook (e.g. auto-send media for generate_image).
			if a.onToolResult != nil && result.Error == nil {
				a.onToolResult(result.Name, result)
			}
		}

		// Inject deferred loop warning AFTER tool results (valid message order:
		// assistant‚Üítool‚Üíuser). This ensures providers that validate message
		// sequences don't reject the request.
		if loopWarning != "" {
			messages = append(messages, chatMessage{
				Role:    "user",
				Content: "[System] " + loopWarning,
			})
		}
	}
}

// formatToolProgressMessage creates a clean, concise, user-facing message about
// what the agent is doing. Designed for chat apps (WhatsApp, Telegram).
// Unlike step-by-step output, this shows a single summarized line.
// Format: emoji + label + optional detail.
func formatToolProgressMessage(toolCalls []ToolCall) string {
	if len(toolCalls) == 0 {
		return ""
	}

	// For a single tool call, show a concise description.
	if len(toolCalls) == 1 {
		name := toolCalls[0].Function.Name
		args, _ := parseToolArgs(toolCalls[0].Function.Arguments)
		return describeToolAction(name, args)
	}

	// For multiple parallel tool calls, summarize them.
	// Show the most interesting one (longest description) and count the rest.
	var best string
	count := 0
	for _, tc := range toolCalls {
		name := tc.Function.Name
		args, _ := parseToolArgs(tc.Function.Arguments)
		desc := describeToolAction(name, args)
		if desc != "" {
			count++
			if len(desc) > len(best) {
				best = desc
			}
		}
	}

	if count == 0 {
		return ""
	}
	if count == 1 {
		return best
	}
	return fmt.Sprintf("%s (+%d)", best, count-1)
}

// describeToolAction returns a human-friendly, emoji-prefixed description
// of a tool call. Empty string means "skip this tool in progress output".
func describeToolAction(name string, args map[string]any) string {
	switch name {
	// ‚îÄ‚îÄ Shell / commands ‚îÄ‚îÄ
	case "bash", "exec":
		cmd, _ := args["command"].(string)
		if cmd == "" {
			return "üíª Executando comando..."
		}
		if len(cmd) > 60 {
			cmd = cmd[:60] + "..."
		}
		return "üíª `" + cmd + "`"

	// ‚îÄ‚îÄ File operations ‚îÄ‚îÄ
	case "read_file":
		p, _ := args["path"].(string)
		if p != "" {
			return "üìñ Lendo " + shortPath(p)
		}
		return "üìñ Lendo arquivo..."

	case "write_file":
		p, _ := args["path"].(string)
		if p != "" {
			return "‚úçÔ∏è Escrevendo " + shortPath(p)
		}
		return "‚úçÔ∏è Escrevendo arquivo..."

	case "edit_file":
		p, _ := args["path"].(string)
		if p != "" {
			return "‚úèÔ∏è Editando " + shortPath(p)
		}
		return "‚úèÔ∏è Editando arquivo..."

	case "list_files", "glob_files":
		p, _ := args["path"].(string)
		if p == "" {
			p, _ = args["pattern"].(string)
		}
		if p != "" {
			return "üìÇ Listando " + shortPath(p)
		}
		return "üìÇ Listando arquivos..."

	case "search_files":
		q, _ := args["query"].(string)
		if q == "" {
			q, _ = args["pattern"].(string)
		}
		if q != "" {
			return "üîé Buscando: " + q
		}
		return "üîé Buscando nos arquivos..."

	// ‚îÄ‚îÄ Web ‚îÄ‚îÄ
	case "web_search", "brave-search_execute", "brave-search_run_search":
		q, _ := args["query"].(string)
		if q != "" {
			if len(q) > 60 {
				q = q[:60] + "..."
			}
			return "üîç Pesquisando: " + q
		}
		return "üîç Pesquisando na web..."

	case "web_fetch", "web-fetch_fetch_url":
		u, _ := args["url"].(string)
		if u != "" {
			if len(u) > 55 {
				u = u[:55] + "..."
			}
			return "üåê Acessando " + u
		}
		return "üåê Acessando p√°gina..."

	// ‚îÄ‚îÄ Memory ‚îÄ‚îÄ
	case "memory":
		action, _ := args["action"].(string)
		switch action {
		case "save":
			return "üíæ Saving to memory..."
		case "search":
			q, _ := args["query"].(string)
			if q != "" {
				return "üß† Recalling: " + q
			}
			return "üß† Searching memory..."
		case "list", "index":
			return "üß† Organizing memories..."
		default:
			return "üß† Memory..."
		}

	// ‚îÄ‚îÄ Remote ‚îÄ‚îÄ
	case "ssh":
		host, _ := args["host"].(string)
		cmd, _ := args["command"].(string)
		if host != "" && cmd != "" {
			if len(cmd) > 40 {
				cmd = cmd[:40] + "..."
			}
			return "üîó " + host + ": `" + cmd + "`"
		}
		if host != "" {
			return "üîó Conectando em " + host + "..."
		}
		return "üîó Conectando via SSH..."

	case "scp":
		src, _ := args["source"].(string)
		dst, _ := args["destination"].(string)
		if src != "" && dst != "" {
			return "üì§ Transferindo " + shortPath(src) + " ‚Üí " + shortPath(dst)
		}
		return "üì§ Transferindo arquivo..."

	// ‚îÄ‚îÄ Coding ‚îÄ‚îÄ
	case "claude-code_execute":
		p, _ := args["prompt"].(string)
		if p != "" {
			if len(p) > 55 {
				p = p[:55] + "..."
			}
			return "ü§ñ Codificando: " + p
		}
		return "ü§ñ Executando Claude Code..."
	case "claude-code_check":
		return "ü§ñ Verificando Claude Code..."

	// ‚îÄ‚îÄ Images ‚îÄ‚îÄ
	case "describe_image":
		return "üëÅÔ∏è Analisando imagem..."
	case "image-gen_generate_image":
		p, _ := args["prompt"].(string)
		if p != "" {
			if len(p) > 50 {
				p = p[:50] + "..."
			}
			return "üé® Gerando imagem: " + p
		}
		return "üé® Gerando imagem..."

	// ‚îÄ‚îÄ Audio ‚îÄ‚îÄ
	case "transcribe_audio":
		return "üé§ Transcrevendo √°udio..."

	// ‚îÄ‚îÄ Scheduler ‚îÄ‚îÄ
	case "cron_add":
		return "‚è∞ Criando agendamento..."
	case "cron_list":
		return "‚è∞ Listando agendamentos..."
	case "cron_remove":
		return "‚è∞ Removendo agendamento..."

	// ‚îÄ‚îÄ Vault ‚îÄ‚îÄ
	case "vault_save":
		return "üîê Salvando no cofre..."
	case "vault_get":
		return "üîê Buscando no cofre..."
	case "vault_list":
		return "üîê Listando cofre..."

	// ‚îÄ‚îÄ Skills ‚îÄ‚îÄ
	case "install_skill":
		s, _ := args["name"].(string)
		if s != "" {
			return "üì¶ Instalando skill: " + s
		}
		return "üì¶ Instalando skill..."
	case "list_skills", "search_skills":
		return "üìã Listando skills..."

	// ‚îÄ‚îÄ Subagents ‚îÄ‚îÄ
	case "spawn_subagent":
		label, _ := args["label"].(string)
		if label == "" {
			label, _ = args["task"].(string)
			if len(label) > 40 {
				label = label[:40] + "..."
			}
		}
		if label != "" {
			return "üßµ Iniciando subagente: " + label
		}
		return "üßµ Iniciando subagente..."
	case "list_subagents":
		return "üßµ Verificando subagentes..."
	case "wait_subagent":
		return "‚è≥ Aguardando subagente..."
	case "stop_subagent":
		return "üõë Parando subagente..."

	// ‚îÄ‚îÄ Project Manager ‚îÄ‚îÄ
	case "project-manager_activate":
		p, _ := args["name"].(string)
		if p != "" {
			return "üìÅ Ativando projeto: " + p
		}
		return "üìÅ Ativando projeto..."
	case "project-manager_list":
		return "üìÅ Listando projetos..."
	case "project-manager_scan", "project-manager_tree":
		return "üìÅ Escaneando projeto..."
	case "project-manager_register":
		return "üìÅ Registrando projeto..."

	// ‚îÄ‚îÄ Calculator / DateTime ‚îÄ‚îÄ
	case "calculator_calculate":
		return "" // silent ‚Äî too trivial
	case "datetime_current_time":
		return "" // silent

	default:
		// For skill tools (prefixed), make it cleaner.
		if strings.Contains(name, "_execute") {
			skillName := strings.TrimSuffix(name, "_execute")
			skillName = strings.ReplaceAll(skillName, "_", " ")
			skillName = strings.ReplaceAll(skillName, "-", " ")
			return "‚ö° Executando " + skillName + "..."
		}
		if strings.Contains(name, "_run_") {
			parts := strings.SplitN(name, "_run_", 2)
			skillName := strings.ReplaceAll(parts[0], "-", " ")
			action := strings.ReplaceAll(parts[1], "_", " ")
			return "‚ö° " + skillName + ": " + action + "..."
		}
		return "‚öôÔ∏è " + strings.ReplaceAll(name, "_", " ") + "..."
	}
}

// shortPath returns the last 2 segments of a path for display.
// "/home/user/projects/app/src/index.ts" ‚Üí "src/index.ts"
func shortPath(p string) string {
	parts := strings.Split(p, "/")
	if len(parts) <= 2 {
		return p
	}
	return strings.Join(parts[len(parts)-2:], "/")
}

// isRecoverableToolError checks if a tool error is likely transient or due to
// incorrect parameters, so the model should retry without surfacing it to the user.
// Classifies errors that the model can recover from by retrying or adjusting parameters.
func isRecoverableToolError(errMsg string) bool {
	lower := strings.ToLower(errMsg)
	patterns := []string{
		"required",       // "path is required", "prompt is required"
		"missing",        // "missing parameter"
		"not found",      // "file not found" (model can fix path)
		"invalid",        // "invalid argument"
		"parsing",        // "error parsing arguments"
		"no such file",   // fs errors
		"does not exist", // resource not found
		"permission denied",
		"timed out", // transient timeout
		"connection refused",
		"empty", // "command is empty"
	}
	for _, p := range patterns {
		if strings.Contains(lower, p) {
			return true
		}
	}
	return false
}

// truncateStr truncates a string to n characters for logging.
func truncateStr(s string, n int) string {
	if len(s) <= n {
		return s
	}
	return s[:n] + "..."
}

// drainInterrupts reads all pending messages from the interrupt channel
// without blocking. Returns nil if no messages are available.
func (a *AgentRun) drainInterrupts() []string {
	if a.interruptCh == nil {
		return nil
	}
	var msgs []string
	for {
		select {
		case msg, ok := <-a.interruptCh:
			if !ok {
				return msgs // Channel closed.
			}
			msgs = append(msgs, msg)
		default:
			return msgs
		}
	}
}

// accumulateUsage adds resp.Usage into total.
func (a *AgentRun) accumulateUsage(total *LLMUsage, resp *LLMResponse) {
	if resp == nil {
		return
	}
	total.PromptTokens += resp.Usage.PromptTokens
	total.CompletionTokens += resp.Usage.CompletionTokens
	total.TotalTokens += resp.Usage.TotalTokens
}

// buildMessages converts conversation history into the chat message format.
func (a *AgentRun) buildMessages(systemPrompt string, history []ConversationEntry, userMessage string) []chatMessage {
	messages := make([]chatMessage, 0, len(history)*2+2)

	if systemPrompt != "" {
		messages = append(messages, chatMessage{
			Role:    "system",
			Content: systemPrompt,
		})
	}

	for _, entry := range history {
		messages = append(messages, chatMessage{
			Role:    "user",
			Content: entry.UserMessage,
		})
		if entry.AssistantResponse != "" {
			messages = append(messages, chatMessage{
				Role:    "assistant",
				Content: entry.AssistantResponse,
			})
		}
	}

	messages = append(messages, chatMessage{
		Role:    "user",
		Content: userMessage,
	})

	return messages
}

// isContextOverflow checks if an error indicates context length exceeded.
func isContextOverflow(err error) bool {
	if err == nil {
		return false
	}
	s := strings.ToLower(err.Error())
	return strings.Contains(s, "context_length_exceeded") ||
		strings.Contains(s, "maximum context length") ||
		(strings.Contains(s, "400") && strings.Contains(s, "tokens"))
}

// compactMessages removes older messages to reduce context size.
// Keeps: system prompt (first), last N messages.
func (a *AgentRun) compactMessages(messages []chatMessage, keepRecent int) []chatMessage {
	if len(messages) <= keepRecent+1 {
		return messages
	}

	var result []chatMessage
	// Always keep the first message if it's system.
	if len(messages) > 0 && messages[0].Role == "system" {
		result = append(result, messages[0])
		// Keep last keepRecent from the rest.
		rest := messages[1:]
		if len(rest) <= keepRecent {
			result = append(result, rest...)
			return result
		}
		// Find safe cut point to avoid orphan tool messages
		cutIdx := len(rest) - keepRecent
		cutIdx = a.findSafeCutPoint(rest, cutIdx)
		rest = rest[cutIdx:]
		result = append(result, rest...)
	} else {
		// No system message; keep last keepRecent.
		if len(messages) <= keepRecent {
			return messages
		}
		cutIdx := len(messages) - keepRecent
		cutIdx = a.findSafeCutPoint(messages, cutIdx)
		result = make([]chatMessage, len(messages)-cutIdx)
		copy(result, messages[cutIdx:])
	}
	return result
}

// findSafeCutPoint ensures we don't cut in the middle of a tool call/result sequence.
// It returns an adjusted index that points to a user or assistant message (not tool).
func (a *AgentRun) findSafeCutPoint(messages []chatMessage, proposedIdx int) int {
	// Walk backward from proposed index to find a safe starting point
	for i := proposedIdx; i > 0; i-- {
		if messages[i].Role == "user" || (messages[i].Role == "assistant" && len(messages[i].ToolCalls) == 0) {
			// Found a safe starting point (user message or assistant without pending tool calls)
			return i
		}
		if messages[i].Role == "assistant" && len(messages[i].ToolCalls) > 0 {
			// This assistant has tool calls, so we need to include it
			return i
		}
		// If it's a tool message, keep going backward
	}
	return proposedIdx
}

// truncateToolResults shortens tool result messages that exceed maxLen.
func (a *AgentRun) truncateToolResults(messages []chatMessage, maxLen int) []chatMessage {
	if maxLen <= 0 {
		maxLen = 2000
	}
	truncSuffix := "... [truncated]"
	keepChars := 1000
	if keepChars+len(truncSuffix) > maxLen {
		keepChars = maxLen - len(truncSuffix)
	}

	result := make([]chatMessage, len(messages))
	for i, m := range messages {
		result[i] = m
		if m.Role == "tool" {
			if s, ok := m.Content.(string); ok && len(s) > maxLen {
				result[i].Content = s[:keepChars] + truncSuffix
			}
		}
	}
	return result
}

// pruneOldToolResults implements proactive context trimming.
// Tool results are tagged with their turn number. Older results are progressively
// truncated or removed to keep the context lean without waiting for overflow.
func (a *AgentRun) pruneOldToolResults(messages []chatMessage, currentTurn int) []chatMessage {
	const (
		softTrimAge   = 5   // Turns before soft trim (truncate to 500 chars)
		hardTrimAge   = 10  // Turns before hard trim (remove entirely)
		softTrimChars = 500 // Max chars after soft trim
	)

	// Estimate the "turn" of each message based on position. Tool messages
	// between two assistant messages belong to the same turn.
	msgCount := len(messages)
	if msgCount < 10 {
		return messages
	}

	// Count tool result messages from the end to estimate age.
	toolResultCount := 0
	for i := len(messages) - 1; i >= 0; i-- {
		if messages[i].Role == "tool" {
			toolResultCount++
		}
	}

	if toolResultCount < softTrimAge {
		return messages // Not enough tool results to prune.
	}

	// Walk through messages, trimming old tool results.
	result := make([]chatMessage, 0, len(messages))
	toolIdx := 0
	for _, m := range messages {
		if m.Role == "tool" {
			age := toolResultCount - toolIdx
			toolIdx++

			if age > hardTrimAge {
				// Hard trim: skip this tool result entirely.
				result = append(result, chatMessage{
					Role:       m.Role,
					Content:    "[tool result removed ‚Äî too old]",
					ToolCallID: m.ToolCallID,
				})
				continue
			}

			if age > softTrimAge {
				// Soft trim: truncate to 500 chars.
				if s, ok := m.Content.(string); ok && len(s) > softTrimChars {
					m.Content = s[:softTrimChars] + "... [truncated ‚Äî old result]"
				}
			}
		}
		result = append(result, m)
	}

	return result
}

// doLLMCallWithOverflowRetry runs the LLM call and retries with compaction on context overflow.
// The per-call timeout is a safety net (llmCallTimeout, default 5min) ‚Äî the primary timeout
// is the run-level context passed in ctx.
//
// Compaction strategy:
//  1. First attempt: truncate oversized tool results (>4K chars).
//  2. Second attempt: compact messages (keep last N) + truncate tool results harder.
//  3. Third attempt: aggressive compaction (keep fewer messages).
func (a *AgentRun) doLLMCallWithOverflowRetry(ctx context.Context, messages []chatMessage, tools []ToolDefinition) (*LLMResponse, error) {
	toolResultTruncated := false

	for attempt := 0; attempt < a.maxCompactionAttempts; attempt++ {
		// Use the shorter of: run context deadline or llmCallTimeout safety net.
		callCtx, cancel := context.WithTimeout(ctx, a.llmCallTimeout)
		var resp *LLMResponse
		var err error
		if a.streamCallback != nil {
			resp, err = a.llm.CompleteWithToolsStreamUsingModel(callCtx, a.modelOverride, messages, tools, a.streamCallback)
		} else {
			resp, err = a.llm.CompleteWithFallbackUsingModel(callCtx, a.modelOverride, messages, tools)
		}
		cancel()

		if err == nil {
			if a.usageRecorder != nil && resp.Usage.TotalTokens > 0 {
				a.usageRecorder(resp.ModelUsed, resp.Usage)
			}
			return resp, nil
		}

		if !isContextOverflow(err) {
			return nil, err
		}

		a.logger.Info("context overflow detected",
			"attempt", attempt+1,
			"max_attempts", a.maxCompactionAttempts,
			"messages_before", len(messages),
		)

		// ‚îÄ‚îÄ Compaction strategy ‚îÄ‚îÄ
		// Step 1: Try truncating oversized tool results first (cheap operation).
		if !toolResultTruncated {
			if hasOversizedToolResults(messages, 4000) {
				a.logger.Info("truncating oversized tool results before compaction")
				messages = a.truncateToolResults(messages, 4000)
				toolResultTruncated = true
				continue // Retry without compacting messages.
			}
		}

		// Step 2+3: Compact messages using LLM summarization.
		a.logger.Info("performing managed compaction due to overflow")
		if attempt == 0 { // First compaction attempt after initial truncation
			messages = a.managedCompaction(ctx, messages)
		} else { // Subsequent attempts, use aggressive compaction
			messages = a.aggressiveCompaction(ctx, messages)
		}
	}

	return nil, fmt.Errorf("context overflow: compacted %d times but still exceeded context limit", a.maxCompactionAttempts)
}

// hasOversizedToolResults checks if any tool result message exceeds maxLen.
func hasOversizedToolResults(messages []chatMessage, maxLen int) bool {
	for _, m := range messages {
		if m.Role == "tool" {
			if s, ok := m.Content.(string); ok && len(s) > maxLen {
				return true
			}
		}
	}
	return false
}

// ----------------------------------------------------------------------------
// Compaction logic
// ----------------------------------------------------------------------------

// managedCompaction takes the current message history and safely compacts the older half
// of the conversation into a summary block, preserving recent context and system prompts.
func (a *AgentRun) managedCompaction(ctx context.Context, messages []chatMessage) []chatMessage {
	if len(messages) < 10 {
		return messages
	}

	// Keep system prompt(s) and the first user message (usually the goal)
	var header []chatMessage
	var body []chatMessage

	for _, m := range messages {
		if m.Role == "system" {
			header = append(header, m)
		} else {
			body = append(body, m)
		}
	}

	if len(body) < 8 {
		return messages
	}

	goal := body[0]

	// We want to compact the middle section and keep the most recent N messages
	keepRecent := 6
	if len(body)-1 <= keepRecent {
		return messages // Too small to compact
	}

	// Find safe cut point to avoid orphan tool messages
	cutIdx := len(body) - keepRecent
	cutIdx = a.findSafeCutPoint(body, cutIdx)

	middle := body[1:cutIdx]
	recent := body[cutIdx:]

	summary := a.summarizeMiddle(ctx, middle)

	var compacted []chatMessage
	compacted = append(compacted, header...)
	compacted = append(compacted, goal)
	compacted = append(compacted, chatMessage{
		Role:    "user",
		Content: "[System: The following is a summary of earlier steps. " + summary + "]",
	})
	compacted = append(compacted, recent...)

	a.logger.Info("context compacted",
		"original_len", len(messages),
		"compacted_len", len(compacted),
	)

	return compacted
}

// aggressiveCompaction is used when the context still overflows despite managed compaction.
// It cuts the recent context even shorter and truncates large text heavily.
func (a *AgentRun) aggressiveCompaction(ctx context.Context, messages []chatMessage) []chatMessage {
	// First run standard truncation on tool results
	truncated := a.truncateToolResults(messages, 1500)

	// Then run managed compaction but with a much shorter keepRecent window (done inline to force it)
	var header []chatMessage
	var body []chatMessage
	for _, m := range truncated {
		if m.Role == "system" {
			header = append(header, m)
		} else {
			body = append(body, m)
		}
	}

	if len(body) < 4 {
		return truncated
	}

	goal := body[0]
	keepRecent := 2 // Keep only the absolute minimum to recover

	var summary string
	if len(body)-1 > keepRecent {
		middle := body[1 : len(body)-keepRecent]
		summary = a.summarizeMiddle(ctx, middle)
	} else {
		summary = "History was aggressively truncated."
	}

	var compacted []chatMessage
	compacted = append(compacted, header...)
	compacted = append(compacted, goal)
	if summary != "" {
		compacted = append(compacted, chatMessage{
			Role:    "user",
			Content: "[System: Aggressive fallback compaction of earlier steps. " + summary + "]",
		})
	}
	compacted = append(compacted, body[len(body)-keepRecent:]...)

	return compacted
}

func (a *AgentRun) summarizeMiddle(ctx context.Context, middle []chatMessage) string {
	// Build a fast summary prompt
	var textBuilder strings.Builder
	for _, m := range middle {
		role := m.Role
		content := ""
		if s, ok := m.Content.(string); ok {
			// truncate content going into summarizer to avoid inception loops
			if len(s) > 1000 {
				content = s[:1000] + "...(truncated)"
			} else {
				content = s
			}
		}

		if len(m.ToolCalls) > 0 {
			info := "Used tools: "
			for _, tc := range m.ToolCalls {
				info += tc.Function.Name + ", "
			}
			content += " " + info
		}

		textBuilder.WriteString(fmt.Sprintf("[%s]: %s\n", role, content))
	}

	prompt := []chatMessage{
		{
			Role:    "system",
			Content: "You are a summarizing assistant. Your job is to read a truncated transcript of an agent's past actions and summarize what was attempted, what the results were, and what the current status is. Keep your summary extremely concise (max 3-4 sentences). Focus on facts, failures, and discoveries. NEVER use text formatting like bold or headers.",
		},
		{
			Role:    "user",
			Content: "Summarize this history:\n\n" + textBuilder.String(),
		},
	}

	// Make a quick call using the fast model (e.g., flash/haiku equivalent) if available
	// For now we use the standard model but without tools
	sumCtx, cancel := context.WithTimeout(ctx, 30*time.Second)
	defer cancel()

	resp, err := a.llm.CompleteWithFallbackUsingModel(sumCtx, "", prompt, nil)
	if err != nil {
		a.logger.Warn("compaction summary failed", "error", err)
		return "Failed to summarize earlier steps due to error."
	}

	return resp.Content
}
